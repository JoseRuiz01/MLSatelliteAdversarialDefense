{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef115af",
   "metadata": {},
   "source": [
    "# PGD attacks\n",
    "\n",
    "This notebook tests **EuroSAT ResNet18 model** on images modified with **PGD attacks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d9f70",
   "metadata": {},
   "source": [
    "### 1. Setup environment and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d449ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "if not hasattr(sys, \"frozen\"):\n",
    "    os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.transform import resize\n",
    "\n",
    "from src.training.simple_cnn import SimpleCNN\n",
    "from src.data.dataloader import get_dataloaders\n",
    "from src.attacks.evaluate import evaluate_pgd             \n",
    "from src.attacks.metrics_eval import evaluate_adv, plot_confusion_matrix  \n",
    "from src.attacks.utils import select_rgb_bands, gdal_style_scale\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"resnet18\" # or resnet18 simplecnn\n",
    "data_dir = '../data/raw'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e182d54",
   "metadata": {},
   "source": [
    "- Get path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4903627",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"simplecnn\":\n",
    "        checkpoint_path = \"../experiments/checkpoints/simplecnn_best.pth\"\n",
    "elif model_name == \"resnet18\":\n",
    "        checkpoint_path = \"../experiments/checkpoints/resnet18_best.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f49e1",
   "metadata": {},
   "source": [
    "### 2. Load data and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b611e",
   "metadata": {},
   "source": [
    "- Load dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d388079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 classes: ['Forest', 'Residential', 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, classes = get_dataloaders(data_dir=data_dir, batch_size=batch_size)\n",
    "print(f'Loaded {len(classes)} classes: {classes}')\n",
    "\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac97b7d",
   "metadata": {},
   "source": [
    "- Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b269473",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name.lower() == \"resnet18\":\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "elif model_name.lower() == \"simplecnn\":\n",
    "    model = SimpleCNN(num_classes=num_classes)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model_name: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e04b4",
   "metadata": {},
   "source": [
    "- Load trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6e8601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a616a35",
   "metadata": {},
   "source": [
    "### 3. Run PGD attacks and save adversarial images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71445b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseantonioruizheredia/Code/Python/ml-satellite-adv/src/attacks/utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(t.mean).view(-1, 1, 1)\n",
      "/Users/joseantonioruizheredia/Code/Python/ml-satellite-adv/src/attacks/utils.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(t.std).view(-1, 1, 1)\n",
      "PGD eps=0.001:  41%|████      | 9/22 [09:33<14:03, 64.87s/it]"
     ]
    }
   ],
   "source": [
    "out_dir = '../data/pgd'\n",
    "\n",
    "eps = 0.001\n",
    "alpha = eps / 10\n",
    "iters = 100\n",
    "\n",
    "res = evaluate_pgd(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    eps=eps,\n",
    "    alpha=alpha,\n",
    "    iters=iters,\n",
    "    out_dir=out_dir,\n",
    "    save_every=20,\n",
    "    max_save=100,\n",
    "    targeted=False,\n",
    "    target_class=None\n",
    ")\n",
    "\n",
    "print('PGD run result:', res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941db19",
   "metadata": {},
   "source": [
    "### 4. Evaluate saved adversarial images for each epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006de99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6ad9f",
   "metadata": {},
   "source": [
    "- Evaluation of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== Model Evaluation on Adversarial Images ===\")\n",
    "\n",
    "metrics_adv = evaluate_adv(\n",
    "    adv_folder=folder,\n",
    "    model_path=checkpoint_path,\n",
    "    data_dir=data_dir,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    mean_std_sample_size=2000,\n",
    "    image_pattern=\"*.tif\"\n",
    ")\n",
    "\n",
    "print(f\"Num images: {metrics_adv['num_images']}\")\n",
    "\n",
    "print(f\"Accuracy: {metrics_adv['accuracy']*100:.2f}%\")\n",
    "print(f\"Loss: {metrics_adv['loss']:.4f}\")\n",
    "print(f\"Precision: {metrics_adv['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_adv['recall']:.4f}\")\n",
    "print(f\"F1-score: {metrics_adv['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nClassification metrics per category:\\n\\n\", metrics_adv[\"classification_report\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa240e1",
   "metadata": {},
   "source": [
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics_adv['confusion_matrix'], metrics_adv['class_names'], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f19d1",
   "metadata": {},
   "source": [
    "- Show a small sample of images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_folder = '../data/pgd'\n",
    "raw_folder = '../data/raw'\n",
    "adv_paths = sorted(glob.glob(os.path.join(adv_folder, \"*.tif\")))\n",
    "\n",
    "pairs = []\n",
    "for adv_p in adv_paths:\n",
    "    base = os.path.basename(adv_p)\n",
    "    orig_base = re.sub(r\"_true\\d+_pred\\d+\\.tif$\", \".tif\", base)\n",
    "    orig_candidates = glob.glob(os.path.join(raw_folder, \"**\", orig_base), recursive=True)\n",
    "    if not orig_candidates:\n",
    "        continue\n",
    "    pairs.append((adv_p, orig_candidates[0]))\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    print(\"No matching pairs found.\")\n",
    "else:\n",
    "    random.shuffle(pairs)\n",
    "    pairs = pairs[:10]\n",
    "    fig, axs = plt.subplots(len(pairs), 3, figsize=(12, 4 * len(pairs)), dpi=400)\n",
    "    axs = np.atleast_2d(axs)\n",
    "\n",
    "    for i, (adv_p, orig_p) in enumerate(pairs):\n",
    "        adv = tifffile.imread(adv_p)\n",
    "        orig = tifffile.imread(orig_p)\n",
    "        raw_diff = np.abs(adv.astype(np.float32) - orig.astype(np.float32))\n",
    "\n",
    "        if adv.ndim == 3 and adv.shape[0] in [3,4,13]:\n",
    "            adv = np.transpose(adv, (1,2,0))\n",
    "        if orig.ndim == 3 and orig.shape[0] in [3,4,13]:\n",
    "            orig = np.transpose(orig, (1,2,0))\n",
    "\n",
    "        adv_rgb = select_rgb_bands(adv)\n",
    "        orig_rgb = select_rgb_bands(orig)\n",
    "        if adv_rgb.shape != orig_rgb.shape:\n",
    "            orig_rgb = resize(orig_rgb, adv_rgb.shape, preserve_range=True, anti_aliasing=True)\n",
    "\n",
    "        adv_disp = gdal_style_scale(adv_rgb)\n",
    "        orig_disp = gdal_style_scale(orig_rgb)\n",
    "\n",
    "        \n",
    "        diff = np.mean(np.abs(adv_disp - orig_disp), axis=2)\n",
    "\n",
    "        p99 = np.percentile(diff, 99)\n",
    "        diff_clipped = np.clip(diff / (p99 + 1e-12), 0, 1)\n",
    "        scale = 0.01\n",
    "        diff_vis = np.log1p(diff_clipped * scale) / np.log1p(scale)\n",
    "\n",
    "        ax0, ax1, ax2 = axs[i]\n",
    "        ax0.imshow(orig_disp)\n",
    "        ax0.set_title(\"Original (normalized)\")\n",
    "        ax0.axis(\"off\")\n",
    "\n",
    "        ax1.imshow(adv_disp)\n",
    "        ax1.set_title(\"Adversarial (normalized)\")\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        im = ax2.imshow(diff_vis, cmap='hot_r', interpolation='nearest')\n",
    "        ax2.set_title(f\"Diff heatmap\\n{os.path.basename(adv_p)}\", fontsize=9)\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "        ax2.text(\n",
    "            0.99, 0.01,\n",
    "            f\"Normalize diff: {diff.mean():.4f}\\nRaw diff: {raw_diff.mean():.4f}\",\n",
    "            color='white',\n",
    "            fontsize=9,\n",
    "            ha='right',\n",
    "            va='bottom',\n",
    "            transform=ax2.transAxes,\n",
    "            bbox=dict(facecolor='black', alpha=0.8, pad=2)\n",
    "        )\n",
    "\n",
    "        cbar = fig.colorbar(im, ax=ax2, fraction=0.046, pad=0.02)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
